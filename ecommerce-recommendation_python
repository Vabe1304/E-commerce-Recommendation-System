# -*- coding: utf-8 -*-
"""proyecto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e1t1EETzk_uxMqX1uhuuPN_m8WMYM8AP

# Sistema de recomendación
## Análisis predictivo de comportamiento de usuario.
### Descripción:
En la actualidad, los sistemas de recomendación desempeñan un papel crucial en la personalización de la experiencia del usuario en diversas plataformas, desde tiendas en línea hasta servicios de streaming. Un sistema de recomendación eficiente no solo mejora la experiencia del usuario, sino que también incrementa las tasas de conversión y fidelización.

### Objetivo:
El objetivo es analizar y predecir el comportamiento de los usuarios utilizando el dataset de Kaggle. Con esta información, se busca responder preguntas clave sobre la confiabilidad de las recomendaciones, la selección óptima de artículos recomendados y las tendencias de interacción de los usuarios en diferentes plataformas.

### Nombres:
- Valentina Agudelo Echeverri

##Dashboard:
[https://lookerstudio.google.com/reporting/f4cc7c34-d186-42b4-8183-4118c97c4a90/page/p_bfh1uzr0qd]

# Exploración y limpieza
"""

!pip install findspark

import kagglehub
import os
import gc
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Descargar el dataset
path = kagglehub.dataset_download("mkechinov/ecommerce-events-history-in-electronics-store")
print("Ruta a los archivos:", path)

# Archivo csv con los datos
dataset_path = "/root/.cache/kagglehub/datasets/mkechinov/ecommerce-events-history-in-electronics-store/versions/1"

# Leer el dataset
file_path = f"{dataset_path}/events.csv"
df = pd.read_csv(file_path)

df

# Porcentaje de valores nulos
df.isnull().sum() / len(df) * 100

df.isnull()

# Registros antes de eliminar nulos
print(df.shape)

# Eliminar registros nulos
df.dropna(inplace=True)
df = df.reset_index(drop=True)
print(df.shape)

df.info()

# Asegurarse de que todos los valores categoricos esten en minusculas
for column in df.select_dtypes(include=['object']).columns:
  df[column] = df[column].str.lower()

df.describe()

# Niveles de las variables categoricas
df.select_dtypes(include='object').nunique()

# Crear los subgraficos
fig, axes = plt.subplots(1, 2, figsize=(16, 6))  # 1 fila, 2 columnas

# Primer grafico: Distribucion de precios con KDE (histograma)
sns.histplot(df['price'], kde=True, bins=100, color="skyblue",
             edgecolor="black", ax=axes[0])

# Anadir titulos
axes[0].set_title('Distribucion de precios', fontsize=16)
axes[0].set_xlabel('Precio', fontsize=14)
axes[0].set_ylabel('Frecuencia', fontsize=14)
axes[0].grid(True)

# Segundo grafico: Distribucion de precios entre 0 y 1000 con KDE (histograma)
sns.histplot(df['price'][df['price'] < 1000], kde=True, bins=50,
             color="green", edgecolor="black", ax=axes[1])
axes[1].set_title('Distribucion de precios (0 - 1000)', fontsize=16)
axes[1].set_xlabel('Precio', fontsize=14)
axes[1].set_ylabel('Frecuencia', fontsize=14)
axes[1].grid(True)

# Mostrar los graficos
plt.tight_layout()
plt.show()

# Resumen estadistico de la columna 'price'
print(df['price'].describe())

# Valores unicos de la columna 'category_code'
len(df['category_code'].unique())

# Separar la columna 'category_code' en: category e item
df.loc[:, "category"] = df["category_code"].map(lambda x: x.split('.')[0])
df.loc[:, 'item'] = df['category_code'].map(lambda x: x.split('.')[-1])

# Eliminar la columna original
df.drop('category_code', axis=1, inplace=True)

# Valores unicos de la nueva columna, category
df['category'].unique()

# Contar las vistas que tienen las marcas (suma de todos sus productos)
vistas_marca = df[df['event_type']=="view"].groupby('brand').count().iloc[:, 0]
vistas_marca.name = 'vistas'
vistas_marca

# Calcular el segundo cuantil del total de vistas
segundo_cuantil = vistas_marca.quantile(0.5)
segundo_cuantil

# Filtrar las marcas dejando aquellas en el cuantil 3 y 4
marcas_validas = vistas_marca[vistas_marca >= segundo_cuantil]
marcas_validas

# Graficar los subniveles de cada variable categorica excepto 'brand'
cols_cat = ['event_type', 'category', 'item']

# Crear figuras
fig, axes = plt.subplots(nrows=len(cols_cat), ncols=1, figsize=(15, 20))
# Ajustar el espacio vertical entre los subplots
fig.subplots_adjust(hspace=0.5)

# Un nuevo grafico para cada variable
for i, col in enumerate(cols_cat):
  sns.countplot(x=col, data=df, ax=axes[i])
  axes[i].set_title(col, fontsize=14)
  axes[i].set_xlabel(col, fontsize=12)
  axes[i].set_ylabel('Frecuencia', fontsize=12)

  # Rotar las etiquetas del eje x para una mejor legibilidad
  plt.setp(axes[i].get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

# Ajustar el diseño de la figura
plt.tight_layout()
plt.show()

"""# Respuesta a Preguntas Descriptivas

## ¿Cuáles son los productos más vendidos?
"""

# Filtramos solo las filas con 'purchase' en event_type
df_buy = df[df['event_type'] == 'purchase']

# Agrupar por 'product_id', contar las ocurrencias y ordenar por 'count'
result = df.groupby(['product_id', 'item', 'category']).size().reset_index(name='count')

# Ordenar por la columna 'count' de manera descendente y tomar los primeros 50 valores
sorted_result = result.sort_values(by='count', ascending=False).head(50)

# Crear una columna combinada con 'category' y 'product_id'
sorted_result['combined'] = '(' + sorted_result['item'].astype(str) + ') ' + sorted_result['product_id'].astype(str)

# Crear el gráfico de barras usando la columna combinada como etiquetas
plt.figure(figsize=(10, 6))
plt.bar(sorted_result['combined'], sorted_result['count'], color='skyblue')

# Etiquetas y titulo
plt.xlabel('Categoria e ID del Producto')
plt.ylabel('Ventas')
plt.title('Conteo de Productos mas vendidos')
plt.xticks(rotation=90)

# Mostrar el grafico
plt.tight_layout()
plt.show()

"""## ¿Qué productos cumplen con el flujo compra? (Productos que pasen por vista, carrito, compra)"""

# Agrupamos por producto
product_events = df.groupby('product_id')['event_type'].unique()

# Productos donde 'event_type' tiene todos los niveles ('view', 'cart', 'purchase')
flow_products = product_events[product_events.apply(
    lambda x: all(event in x for event in ['view', 'cart', 'purchase']))]

# IDs de los productos que tienen el flujo completo
flow_products = flow_products.index.tolist()
# Filtrar el dataframe original sobre esos productos
flow_df = df[df['product_id'].isin(flow_products)]
# Agrupar el df por producto
flow_df = flow_df.groupby(
            ['product_id', 'item', 'category']).size().reset_index(name='count')
flow_df = flow_df.sort_values(by='count', ascending=False).reset_index(drop=True)
flow_df.head(10)

"""## ¿Qué marcas ofrecen los productos más económicos?"""

# Agrupar por marca y calcular el promedio de precios
avg_prices = df.groupby('brand')['price'].mean().reset_index()
# Ordenar el df por precios
avg_prices = avg_prices.sort_values(by=['price'], ascending=True)
avg_prices.head(10)

"""## ¿Qué marcas compran más los usuarios?"""

# Filtrar por compras
purchases = df[df['event_type'] == 'purchase']

# Agrupamos por marca y contamos su total de compras
brand_purchases = purchases.groupby('brand')['product_id'].count().reset_index()

# Ordenamos por 'product_id' que representa el total de compras por marca
brand_purchases = brand_purchases.sort_values(by=['product_id'], ascending=False)
brand_purchases.rename(columns={'product_id': 'purchase_count'}, inplace=True)
brand_purchases = brand_purchases.reset_index(drop=True)

brand_purchases.head(10)

# Descarga de archivos csv
# df.to_csv('df.csv', index=False)
# flow_df.to_csv('flow_df.csv', index=False)
# avg_prices.to_csv('avg_prices.csv', index=False)
# brand_purchases.to_csv('brand_purchases.csv', index=False)

# Limpiar memoria
del flow_df, avg_prices, brand_purchases

"""# Analitica predictiva

## Evaluar integridad de los datos
"""

# user_session es innecesaria
df.drop('user_session', axis=1, inplace=True)

# Numero de interacciones por usuario
interactions_user = df.groupby('user_id').size()

# Numero de interacciones por producto
interactions_product = df.groupby('product_id').size()

print(f"interacciones promedio por usuario: {interactions_user.mean()}")
print(f"interacciones promedio por producto: {interactions_product.mean()}")

# # Optimizar memoria RAM
# # Borrar todas las variables excepto el dataframe
# keep = ['df', 'os', 'gc', 'pd', 'plt', 'sns', 'OneHotEncoder', 'StandardScaler',
#         'SparkSession', 'findspark', 'StorageLevel', 'SparkConf']
# user_defined_vars = [var for var in globals() if not var.startswith("__") and var not in keep]
# for var in user_defined_vars:
#     del globals()[var]

# Interacciones minima
user_interaction = 3

# Filtrar usuarios con minimas interacciones
user_interaction_counts = df['user_id'].value_counts()
users = user_interaction_counts[user_interaction_counts >= user_interaction].index
df_filtered_users = df[df['user_id'].isin(users)].reset_index(drop=True)

# Liberar memoria
del user_interaction_counts, users, df
gc.collect()

print(f"Dimensiones del nuevo conjunto de datos: {df_filtered_users.shape}")

# Volver a revisar los promedios de interacciones
user_interaction_counts = df_filtered_users['user_id'].value_counts()
print(f"Min user interactions: {user_interaction_counts.min()}")
print(f"Max user interactions: {user_interaction_counts.max()}")
print(f"Average user interactions: {user_interaction_counts.mean()}, \n")

product_interaction_counts = df_filtered_users['product_id'].value_counts()
print(f"Min product interactions: {product_interaction_counts.min()}")
print(f"Max product interactions: {product_interaction_counts.max()}")
print(f"Average product interactions: {product_interaction_counts.mean()}")

df_filtered_users.info()

# Eliminar el caracter 'utc' al final de cada fecha de la columna 'event_time'
df_filtered_users['event_time'] = df_filtered_users['event_time'].str.replace(' UTC', '')

"""## Preprocesar los datos"""

import findspark
from pyspark import StorageLevel
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import FloatType
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer
from pyspark.ml.feature import Normalizer, MinMaxScaler
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml.clustering import KMeans
from pyspark.ml.linalg import Vectors

# Instalar y configurar pyspark
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar xf spark-3.4.1-bin-hadoop3.tgz
!pip install -q findspark

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

# Inicializa Spark
findspark.init()

# Crea la sesion de Spark
spark = SparkSession.builder.master("local[*]").appName('content-based-recommendation').getOrCreate()
# Verificar la sesion de Spark
print(spark.sparkContext.getConf().getAll())

# Convertir el DataFrame de Pandas a PySpark
df = spark.createDataFrame(df_filtered_users)

# Verificar los datos cargados
df.show(5)

# Persistir los datos en memoria y disco para minimizar el uso de RAM
df.persist(StorageLevel.MEMORY_AND_DISK)

# Particionar los datos en pequenas porciones
df = df.repartition(10)

# Tomamos el valor mas frecuente para columnas categoricas (category, brand) y el promedio para price
df_grouped = df.groupBy("product_id").agg(
    F.first("category").alias("category"),
    F.first("brand").alias("brand"),
    F.avg("price").alias("avg_price")
)

# Indices y One-Hot Encoding para las columnas categoricas
indexer_category = StringIndexer(inputCol="category", outputCol="category_index")
indexer_brand = StringIndexer(inputCol="brand", outputCol="brand_index")

df_indexed = indexer_category.fit(df_grouped).transform(df_grouped)
df_indexed = indexer_brand.fit(df_indexed).transform(df_indexed)

encoder = OneHotEncoder(inputCols=["category_index", "brand_index"], outputCols=["category_encoded", "brand_encoded"])
df_encoded = encoder.fit(df_indexed).transform(df_indexed)

# Normalizacion del precio promedio (avg_price)
assembler = VectorAssembler(inputCols=["avg_price"], outputCol="price_vector")
df_encoded = assembler.transform(df_encoded)

scaler = MinMaxScaler(inputCol="price_vector", outputCol="price_scaled")
df_normalized = scaler.fit(df_encoded).transform(df_encoded)

# Crear el vector de caracteristicas con todas las columnas relevantes
assembler_features = VectorAssembler(inputCols=["category_encoded", "brand_encoded", "price_scaled"], outputCol="features")
df_features = assembler_features.transform(df_normalized)

"""## Calcular similitudes"""

# id's de productos unicos
df_features.select("product_id").distinct().show(10)

# Obtener la marca y el item del producto objetivo
item_id = 1008798

product_info = df.filter(df.product_id == item_id).select("brand", "item").first()
target_brand = product_info["brand"]
target_item = product_info["item"]

print('Producto seleccionado:', target_item)
print('Marca del producto seleccionado:', target_brand)

# Excluir el producto objetivo y calcular similitud
target_item_df = df_features.filter(df_features.product_id == item_id)

if target_item_df.count() == 0:
    print(f"No se encontró el producto con product_id {item_id}")
else:
    target_item = target_item_df.select("features").collect()[0][0]

    # Funcion para calcular la similitud del coseno
    cosine_similarity = F.udf(lambda x: float(
        x.dot(Vectors.dense(target_item))), FloatType())

    # Calcular la similitud, excluyendo el producto objetivo
    df_similar_items = df_features.withColumn("similarity", cosine_similarity("features")) \
                                  .filter(df_features.product_id != item_id)
    # Cambiar el nombre de la columna 'brand' para evitar ambiguedad
    df_similar_items = df_similar_items.withColumnRenamed("brand", "similar_item_brand")

    # Hacer join con el DataFrame original para obtener los nombres de los productos
    df_names = df_similar_items.join(df.select(
        "product_id", "item", "brand").distinct(), on="product_id")

    # Mostrar todos los productos similares
    df_names.orderBy(F.desc("similarity")).select(
        "product_id", "item", "brand", "similarity").show(10)

# Filtrar para omitar items iguales o de la misma marca
df_filtered = df_names.filter(
    (df_names["brand"] != target_brand) &
    ~(df_names["product_id"].isin([item_id])))

# Mostrar los 10 productos mas similares con el filtro
df_filtered.orderBy(F.desc("similarity")).select(
    "product_id", "item", "brand", "similarity").show(10)

# Tamaño del resultado
print(df_filtered.count())

"""# Modelo ML

## Preprocesar los datos
"""

# Borrar columna reiterativa 'category_id'
df = df.drop('category_id')
df.show(5)

# Descomponer la fecha de la interaccion
df = df.withColumn("month", F.month(df.event_time))
df = df.withColumn("day", F.dayofmonth(df.event_time))
df = df.drop("event_time")
df.show(5)

# Crear un Vector para las columnas numericas
assembler_numeric = VectorAssembler(inputCols=["price", "day"], outputCol="numeric_features")

# Aplicar MinMaxScaler a las características numericas
scaler = MinMaxScaler(inputCol="numeric_features", outputCol="scaled_numeric_features")

# Indexar variables categoricas (event_type, brand, category, item)
indexers = [
    StringIndexer(inputCol="brand", outputCol="brand_index", handleInvalid="keep"),
    StringIndexer(inputCol="category", outputCol="category_index", handleInvalid="keep"),
    StringIndexer(inputCol="item", outputCol="item_index", handleInvalid="keep")
]

# Codificar las variables categoricas
encoders = [
    OneHotEncoder(inputCol="brand_index", outputCol="brand_encoded"),
    OneHotEncoder(inputCol="category_index", outputCol="category_encoded"),
    OneHotEncoder(inputCol="item_index", outputCol="item_encoded")
]

# Ensamblar las columnas categoricas y numericas
assembler = VectorAssembler(
    inputCols=["scaled_numeric_features", "brand_encoded",
               "category_encoded", "item_encoded"],
    outputCol="features"
)

# Paso 3: Agregar el modelo K-Means al pipeline
kmeans = KMeans(k=13, seed=42, featuresCol='features', predictionCol='prediction')

# Crear el pipeline con los pasos anteriores
pipeline = Pipeline(stages=indexers + encoders + [assembler_numeric, scaler, assembler, kmeans])

"""## Entrenar y evaluar el modelo"""

# Entrenar el modelo K-Means en los datos
kmeans_model = pipeline.fit(df)

# Realizar predicciones (asignar clusters)
clusters = kmeans_model.transform(df)

# Mostrar algunas predicciones
clusters.select("brand", "prediction").show(15)

# Contar total de interacciones por cluster
clusters.groupBy('prediction').count().show()

# Evaluar la calidad del clustering usando el Silhouette score
evaluator = ClusteringEvaluator(featuresCol='features')
silhouette = evaluator.evaluate(clusters)

print(f"Silhouette Score: {silhouette}")

## Guardar archivos
# items_similares = df_filtered.orderBy(F.desc("similarity")).select(
#     "product_id", "item", "brand", "similarity")
# items_similares.write.csv("items_similares.csv", header=True)

# interacciones_clusterizadas = clusters.select("brand", "item","prediction")
# interacciones_clusterizadas.write.csv("interacciones_clusterizadas.csv", header=True)
